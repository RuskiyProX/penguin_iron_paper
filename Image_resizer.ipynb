{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb38cb3-10b4-40a3-8cbc-59692a2640e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install patchify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e4cc3f-0a9a-4cc2-92a4-71a1d9cf8827",
   "metadata": {},
   "source": [
    "Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044aa260-187a-4d63-b1f7-1d59cf2fa273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import imagecodecs\n",
    "import os\n",
    "import tifffile as tifi\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from patchify import patchify, unpatchify\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from scipy.spatial import distance\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ead52-8b82-4775-bd7d-27ca0150d828",
   "metadata": {},
   "source": [
    "First, we need to preprocess the image we want to be inferenced by our model. After choosing the image, it is necessary to create small patches, which will be fed to the model. If the patch size is very big, the model may not detect properly all the objects, so just to assure the maximum efficiency, we will use the same patch size as the images we used for training the model: 224x224. \n",
    "\n",
    "To create patches of 224x224, our image must be dividable exactly by these dimensions. For instance, if our original image that we want to be patched is 3000 high and 4000 wide, we can not tile in equal patches without having extra pixel that can not form another patch of 224x224, i.e. 3000%224 is not 0, the same with 4000. \n",
    "\n",
    "In order to have a division with the reaminder = 0, we must either enlarge or shirnk our original image. To enlarge, we add to each dimension the difference between the patch size and the reminder. To shrink, we deduce the reminder from the original size. As our image has enough quality, we will use the enlargement process, expressed by the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a060f4-8373-4e6a-9764-a03f7dd91adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized image created\n"
     ]
    }
   ],
   "source": [
    "class Resizer():\n",
    "    def __init__(self, patch_sz):\n",
    "        self.patch_sz = patch_sz\n",
    "        \n",
    "    def get_correct_shape(self, orig_img_sz, patch_sz):\n",
    "        modified_img_sz = orig_img_sz # we assume that we got the correct image size\n",
    "        extra = orig_img_sz % patch_sz\n",
    "        if extra != 0:\n",
    "            #ENLARGING\n",
    "            modified_img_sz = (orig_img_sz + (patch_sz - extra))\n",
    "            #SHRINKING\n",
    "            #modified_img_sz = (orig_img_sz - extra)\n",
    "        return modified_img_sz\n",
    "\n",
    "    def resize(self, path_img_file, patch_sz):\n",
    "        #Modiffied this to read a large TIF file!\n",
    "        image = tifi.imread(path_img_file)\n",
    "        img = image\n",
    "        #img = cv2.imread(path_img_file) # if color image, then 3D array\n",
    "        orig_img_sz = {\"W\":img.shape[1], \"H\":img.shape[0]}\n",
    "\n",
    "        # what if the image cannot be split into patches perfectly?\n",
    "        # original sz W - 3000, original H sz - 4000\n",
    "        # enlarging the image - (3000 + (224 - 88)) % 224\n",
    "        # shrinking the image - 3000 - 88\n",
    "        modified_img_sz = {} \n",
    "        modified_img_sz[\"H\"] = self.get_correct_shape(orig_img_sz=orig_img_sz[\"H\"], patch_sz=patch_sz[\"H\"]) \n",
    "        modified_img_sz[\"W\"] = self.get_correct_shape(orig_img_sz=orig_img_sz[\"W\"], patch_sz=patch_sz[\"W\"]) \n",
    "        # resize the image\n",
    "        #img = cv2.resize(img, (modified_img_sz[\"W\"], modified_img_sz[\"H\"]), interpolation=cv2.INTER_CUBIC)\n",
    "        img = cv2.resize(img, (modified_img_sz[\"W\"], modified_img_sz[\"H\"]), interpolation=cv2.INTER_CUBIC)\n",
    "        # save the resized image\n",
    "        cv2.imwrite("SAVE_PATH", img)\n",
    "        \n",
    "patch_sz = {\"W\":416, \"H\":416} # This should be dependant on the model input\n",
    "path_img_file = \"PATH_TO_IMAGE_TO_RESIZE\n",
    "patch_maker = Resizer(patch_sz)\n",
    "patch_maker.resize(path_img_file=path_img_file, patch_sz=patch_sz)\n",
    "print('Resized image created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc08da96-3290-4cc3-ab91-8224f245c1d1",
   "metadata": {},
   "source": [
    "Then, after resizing the desired image accordingly, we are going to create the patches using the library patchify. Once the patches are created, we will save them into a folder. This is done by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "214b5083-afe0-4ff6-833f-6350972126ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7540 patches stored in the indicated path\n"
     ]
    }
   ],
   "source": [
    "import imagecodecs\n",
    "image = tifi.imread(\"./patches/AntarcticaII/resized/Test3r.TIF\")\n",
    "img = image\n",
    "#img = cv2.imread(\"./patches/AntarcticaII/resized/Test3r.TIF\")\n",
    "patches_img = patchify(img, (416,416,3), step=416)  # patches_img.shape = (14, 18, 1, 224, 224, 3)\n",
    "y = 0\n",
    "for i in range(patches_img.shape[0]):\n",
    "    for j in range(patches_img.shape[1]):\n",
    "        single_patch_img = patches_img[i, j, 0, :, :, :]\n",
    "        #cv2.rectangle(single_patch_img, (30, 30), (224-30, 224-30), (0, 255, 0), 3)  # Draw something (for testing).\n",
    "        # THIS CODE IS TO DRAW THE POINT IN THE MIDDLE OF THE IMAGE\n",
    "        c1 = (30+194)/2, (30+194)/2\n",
    "        c2 = (30+120)/2, (30+120)/2\n",
    "        rect1center = int(c1[0]), int(c1[1])\n",
    "        rect2center = int(c2[0]), int(c2[1])\n",
    "        # CODE TO GET THE EUCLIDEAN DISTANCE\n",
    "        dst = distance.euclidean(c1, c2)\n",
    "        #cv2.circle(single_patch_img, rect1center, radius=2, color=(255, 0, 0), thickness=3)\n",
    "        #cv2.circle(single_patch_img, rect2center, radius=2, color=(0,0, 255), thickness=3)\n",
    "        #ist = np.linalg.norm(c1-c2)\n",
    "        #print(dst)\n",
    "        if not cv2.imwrite('./patches/AntarcticaII/patches/Det/' + 'image_1_' + str(y).zfill(5) + '.png', single_patch_img):  # Save as PNG, not JPEG for keeping the quality.\n",
    "            raise Exception(\"Could not write the image\")\n",
    "        y += 1\n",
    "\n",
    "num_patches = y\n",
    "\n",
    "# Store an unpatchified reference for testing\n",
    "#cv2.imwrite(\"unpatched_ref.jpg\", unpatchify(patches_img, img.shape))\n",
    "print('{} patches stored in the indicated path'.format(num_patches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c885ae-cb3e-4e5a-83d6-bffcab60a24b",
   "metadata": {},
   "source": [
    "Once the patches are generated, is time to read, perform the inference task and save them back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa254271-8dfc-4afd-874f-f46a63645dd5",
   "metadata": {},
   "source": [
    "First, we load our exported model and the files for the inference::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e5f61-0ab3-49c5-b43a-9ffc93d6ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the exported model from saved_model directory\n",
    "PATH_TO_SAVED_MODEL =r'C:\\TensorFlow\\workspace\\training_demo\\exported-models\\FRCNN_ResNet101_50KstepsROI2\\saved_model'\n",
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "# LOAD SAVED MODEL AND BUILD DETECTION FUNCTION\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done loading the model! Took {} seconds'.format(elapsed_time))\n",
    "\n",
    "# LOAD LABEL MAP DATA\n",
    "PATH_TO_LABELS=r'C:\\TensorFlow\\workspace\\training_demo\\annotations\\test\\Penguin_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "\n",
    "#Image file for inference\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "    Puts image into numpy array of shape (height, width, channels), where channels=3 for RGB to feed into tensorflow graph.\n",
    "    Args:\n",
    "      path: the file path to the image\n",
    "    Returns:\n",
    "      uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    return np.array(cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "    # Run inference\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    output_dict = model_fn(input_tensor)\n",
    "\n",
    "      # All outputs are batches tensors.\n",
    "      # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "      # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key:value[0, :num_detections].numpy() \n",
    "                     for key,value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "      # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "      # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                  output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "                   image.shape[0], image.shape[1])      \n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                           tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f4460-042b-4923-9c2d-a29ae3304812",
   "metadata": {},
   "source": [
    "Once everything is set up, is time to start the inferencing process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e2c1e-5800-41d7-a6d5-dafff2c43173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting min confidence threshold\n",
    "MIN_CONF_THRESH=.4\n",
    "n = 0\n",
    "count_all = 0\n",
    "for image_path in sorted(glob.glob(r'C:\\Users\\usuario\\Inference\\patches\\test4\\*.png')):\n",
    "    image_np = load_image_into_numpy_array(image_path)\n",
    "    output_dict = run_inference_for_single_image(model, image_np)\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np,\n",
    "          output_dict['detection_boxes'],\n",
    "          output_dict['detection_classes'],\n",
    "          output_dict['detection_scores'],\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          max_boxes_to_draw=200,\n",
    "          min_score_thresh=MIN_CONF_THRESH,\n",
    "          skip_labels=True,\n",
    "          skip_scores=True,\n",
    "          line_thickness=3,\n",
    "          mask_alpha=.4,\n",
    "          agnostic_mode=False)\n",
    "    #display(Image.fromarray(image_np))\n",
    "    # This is the way I'm getting my coordinates\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    # get all boxes from an array\n",
    "    max_boxes_to_draw = boxes.shape[0]\n",
    "    # get scores to get a threshold\n",
    "    scores = output_dict['detection_scores']\n",
    "    # iterate over all objects found\n",
    "    count_ind = 0\n",
    "    for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
    "        if scores is None or scores[i] > MIN_CONF_THRESH:\n",
    "            # boxes[i] is the box which will be drawn\n",
    "            class_name = category_index[output_dict['detection_classes'][i]]['name']\n",
    "            count_ind += 1\n",
    "            #print (\"Detected\", boxes[i], class_name)\n",
    "    \n",
    "    #When loading the image into array, we converted the BGR image into RGB, so we convert it back now\n",
    "    im_bgr = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite('patches/test4det/' + 'image_' + str(n).zfill(4) + '.png', im_bgr)\n",
    "    \n",
    "    #Set the penguin counter for each image\n",
    "    n += 1\n",
    "    count_all = count_all + count_ind\n",
    "    completed_per = float(round((n*100)/num_patches,2))\n",
    "                          \n",
    "    print('Analized {} images out of {}, completed {}%, {} detected: {}'.format(n, num_patches, completed_per, class_name, count_all), end='\\r')\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea011406-0422-4e0b-9e76-c755593d6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate spaces for storing the patches\n",
    "img = cv2.imread(\"test3resized.jpg\")  # Read test.jpg just for getting the shape\n",
    "img = np.zeros_like(img)  # Fill with zeros for the example (start from an empty image).\n",
    "\n",
    "# Use patchify just for getting the size. shape = (14, 18, 1, 224, 224, 3)\n",
    "# We could have also used: patches = np.zeros((14, 18, 1, 224, 224, 3), np.uint8)\n",
    "patches = patchify(img, (224,224,3), step=224)\n",
    "\n",
    "# Read each inferred patch and merge them together to create the final image\n",
    "x = 0\n",
    "for i in range(patches.shape[0]):\n",
    "    for j in range(patches.shape[1]):\n",
    "        single_patch_img = cv2.imread('patches/testdetected/' + 'image_' + str(x).zfill(4) + '.png')  # Read a patch image.\n",
    "        if single_patch_img is None:\n",
    "            raise Exception(\"Could not read the image\") \n",
    "        patches[i, j, 0, :, :, :] = single_patch_img.copy()  # Copy single path image to patches\n",
    "        x += 1\n",
    "reconstructed_image = unpatchify(patches, img.shape)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "cv2.putText(reconstructed_image,'Penguins detected: {}'.format(count_all), (300,200), font, 5,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "cv2.imwrite(\"testunpatched.jpg\", reconstructed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e1a77-6f59-453e-92cc-5d1ac5eae05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8458e5b1-1fcb-48d2-b1a5-eafc32e1cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell converts tiffs to png and reverses BGR default cv2 save to RGB mode. DON'T RUN THIS CELL IN THE SCRIPT, NO NEED.\n",
    "for img in glob.glob(r'C:\\Users\\usuario\\Inference\\TFM_Oleg\\AntarcticaII\\TIFFS\\VALIDATED_TIFFS\\TRAINING_IMG\\DETECTED\\*.tif'):\n",
    "    file_name = os.path.basename(img)\n",
    "    image = tifi.imread(img)\n",
    "    image_RGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    name = (os.path.splitext(file_name)[0])\n",
    "    cv2.imwrite('./AntarcticaII/TIFFS/VALIDATED_TIFFS/TRAINING_IMG/DETECTED/'+str(name)+'.jpeg',image_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a4889-2324-4737-befc-cc3fe442583f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
